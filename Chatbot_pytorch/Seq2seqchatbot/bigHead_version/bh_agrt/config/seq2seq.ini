[strings]
# Mode : train, test, serve
mode = train
seq_data = data/seq.data

train_data = data
resource_data = data/corpus_50w_revised_for_bighead.conv

e = E
m = M

model_data = models

[ints]
# vocabulary size 
# 20,000 is a reasonable size
enc_vocab_size = 20000
dec_vocab_size = 20000
embedding_dim = 128
max_length = 20
# typical options : 128, 256, 512, 1024
layer_size = 256
# dataset size limit; typically none : no limit
max_train_data_size =  60000
batch_size = 32
[floats]
min_loss = 0.5

